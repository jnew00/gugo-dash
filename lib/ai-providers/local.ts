import { AIProvider } from './index'

export class LocalLLMProvider implements AIProvider {
  private baseUrl: string
  private model: string

  constructor() {
    // Support both Ollama (default port 11434) and LM Studio (default port 1234)
    this.baseUrl = process.env.LOCAL_LLM_BASE || 'http://127.0.0.1:1234'
    // Use dedicated text model for reply generation
    this.model = process.env.LOCAL_TEXT_MODEL || process.env.LOCAL_LLM_MODEL || 'openai/gpt-oss-20b'
  }

  async generateReply(tweetText: string, tweetAuthor: string): Promise<{ suggestions: string[], isActualAI: boolean, statusMessage: string }> {
    try {
      const prompt = `You are a social media engagement assistant for GUGO, a fitness and crypto community brand. Generate 3 short, engaging replies to this tweet.

Tweet from @${tweetAuthor}: "${tweetText}"

Requirements:
- Keep replies under 280 characters
- Be friendly and engaging
- Include GUGO spirit when appropriate (running, fitness, community)
- Vary the tone (supportive, humorous, insightful)
- DO NOT use hashtags (#)
- Use maximum 2 emojis per reply, and only if they add meaning
- Emojis should be relevant and professional (no spam)

Generate exactly 3 different replies, one per line:`

      console.log('=== LOCAL LLM PROMPT ===')
      console.log('Model:', this.model)
      console.log('Base URL:', this.baseUrl)
      console.log('Tweet Author:', tweetAuthor)
      console.log('Tweet Text:', tweetText)
      console.log('Full Prompt:')
      console.log(prompt)
      console.log('=== END PROMPT ===\n')

      // Try Ollama API format first
      try {
        const response = await fetch(`${this.baseUrl}/api/generate`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            model: this.model,
            prompt: prompt,
            stream: false,
            options: {
              temperature: 0.8,
              max_tokens: 300
            }
          })
        })

        if (response.ok) {
          const data = await response.json()
          const replies = data.response
            .split('\n')
            .filter((line: string) => line.trim().length > 0)
            .slice(0, 3)
            .map((reply: string) => reply
              .replace(/^\d+[\)\.]\s*/, '')  // Remove 1. or 1) patterns
              .replace(/^[-‚Ä¢]\s*/, '')       // Remove bullet points
              .replace(/^[a-c][\)\.]\s*/i, '') // Remove a) or a. patterns
              .trim())

          if (replies.length > 0) {
            return {
              suggestions: replies,
              isActualAI: true,
              statusMessage: `Generated by ${this.model} (Local LLM - Ollama)`
            }
          }
        }
      } catch (ollamaError) {
        console.log('Ollama not available, trying LM Studio format...')
      }

      // Try LM Studio / OpenAI-compatible format
      const lmStudioResponse = await fetch(`${this.baseUrl}/v1/chat/completions`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: this.model,
          messages: [
            {
              role: 'system',
              content: 'You are a social media engagement assistant for GUGO. Generate short, engaging replies. Never use hashtags. Maximum 2 relevant emojis per reply.'
            },
            {
              role: 'user',
              content: prompt
            }
          ],
          temperature: 0.8,
          max_tokens: 300
        })
      })

      if (lmStudioResponse.ok) {
        const data = await lmStudioResponse.json()
        const content = data.choices[0]?.message?.content || ''
        const replies = content
          .split('\n')
          .filter((line: string) => line.trim().length > 0)
          .slice(0, 3)
          .map((reply: string) => reply
            .replace(/^\d+[\)\.]\s*/, '')  // Remove 1. or 1) patterns
            .replace(/^[-‚Ä¢]\s*/, '')       // Remove bullet points
            .replace(/^[a-c][\)\.]\s*/i, '') // Remove a) or a. patterns
            .trim())

        if (replies.length > 0) {
          return {
            suggestions: replies,
            isActualAI: true,
            statusMessage: `Generated by ${this.model} (Local LLM)`
          }
        }
      }

      console.error('Both Ollama and LM Studio formats failed')
      return this.getFallbackResponse()

    } catch (error) {
      console.error('Local LLM error:', error)
      return this.getFallbackResponse()
    }
  }

  private getFallbackResponse(): { suggestions: string[], isActualAI: boolean, statusMessage: string } {
    return {
      suggestions: [
        'Love the energy! Keep running with GUGO! üèÉ‚Äç‚ëÇÔ∏è',
        'This is what the GUGO community is all about!',
        'We just run! Let\'s go! üí™'
      ],
      isActualAI: false,
      statusMessage: `Using fallback templates (Local LLM at ${this.baseUrl} not available)`
    }
  }
}