interface AnalysisResult {
  description: string
  tags: string[]
  isActualAI: boolean
  statusMessage: string
}

export interface ImageAnalysisProvider {
  analyzeMeme(imagePath: string, filename: string): Promise<AnalysisResult>
}

export class LocalImageAnalysisProvider implements ImageAnalysisProvider {
  private baseUrl: string
  private model: string

  constructor() {
    this.baseUrl = process.env.LOCAL_LLM_BASE || 'http://127.0.0.1:1234'
    // Use dedicated vision model for image analysis
    this.model = process.env.LOCAL_VISION_MODEL || process.env.LOCAL_LLM_MODEL || 'moondream-2b-2025-04-14'
  }

  async analyzeMeme(imagePath: string, filename: string): Promise<AnalysisResult> {
    try {
      console.log(`Analyzing meme with local LLM: ${filename}`)

      // Try to use local vision LLM first
      try {
        const fs = require('fs')
        const imageBuffer = fs.readFileSync(imagePath)
        const base64Image = imageBuffer.toString('base64')

        const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            model: this.model,
            messages: [
              {
                role: 'user',
                content: [
                  {
                    type: 'text',
                    text: `Describe this meme image in detail. What's happening visually? What emotions does it convey? When would someone use this meme?`
                  },
                  {
                    type: 'image_url',
                    image_url: {
                      url: `data:image/jpeg;base64,${base64Image}`
                    }
                  }
                ]
              }
            ],
            max_tokens: 500,
            temperature: 0.3
          })
        })

        if (response.ok) {
          const data = await response.json()
          const content = data.choices[0]?.message?.content || ''

          if (content.trim()) {
            // Extract meaningful tags from the description
            const tags = this.extractTagsFromDescription(content, filename)

            return {
              description: content.trim(),
              tags: tags,
              isActualAI: true,
              statusMessage: `Generated by Local LLM (${this.model})`
            }
          } else {
            console.error('Empty response from local LLM')
            // Fall through to fallback
          }
        } else {
          console.error('Local LLM API error:', response.statusText)
          // Fall through to fallback
        }
      } catch (apiError) {
        console.error('Local LLM connection error:', apiError)
        // Fall through to fallback
      }

      // Fallback: Enhanced filename-based analysis
      const fallbackAnalysis = this.generateFallbackAnalysis(filename)

      return {
        description: fallbackAnalysis.description,
        tags: fallbackAnalysis.tags,
        isActualAI: false,
        statusMessage: `Fallback analysis (Local LLM vision not available)`
      }

    } catch (error) {
      console.error('Local image analysis error:', error)
      const fallbackAnalysis = this.generateFallbackAnalysis(filename)

      return {
        description: fallbackAnalysis.description,
        tags: fallbackAnalysis.tags,
        isActualAI: false,
        statusMessage: 'Fallback analysis (Local LLM error)'
      }
    }
  }

  private extractTagsFromDescription(description: string, filename: string): string[] {
    const desc = description.toLowerCase()
    const tags = []

    // Emotion tags
    if (desc.includes('aggressive') || desc.includes('fist') || desc.includes('angry')) tags.push('aggressive', 'angry', 'determined')
    if (desc.includes('happy') || desc.includes('smiling') || desc.includes('cheerful')) tags.push('happy', 'positive', 'cheerful')
    if (desc.includes('sad') || desc.includes('crying') || desc.includes('disappointed')) tags.push('sad', 'disappointed', 'emotional')
    if (desc.includes('confused') || desc.includes('puzzled') || desc.includes('wondering')) tags.push('confused', 'puzzled', 'thinking')
    if (desc.includes('excited') || desc.includes('enthusiastic') || desc.includes('energetic')) tags.push('excited', 'energetic', 'enthusiastic')

    // Action tags
    if (desc.includes('running') || desc.includes('moving') || desc.includes('walking')) tags.push('running', 'moving', 'active')
    if (desc.includes('pointing') || desc.includes('gesture') || desc.includes('hand')) tags.push('pointing', 'gesture', 'hands')
    if (desc.includes('holding') || desc.includes('carrying') || desc.includes('grabbing')) tags.push('holding', 'carrying')
    if (desc.includes('standing') || desc.includes('posing') || desc.includes('posed')) tags.push('standing', 'posing')

    // Visual elements
    if (desc.includes('yellow') || desc.includes('bright')) tags.push('yellow', 'bright')
    if (desc.includes('torn') || desc.includes('ripped') || desc.includes('damaged')) tags.push('torn', 'damaged', 'rough')
    if (desc.includes('cartoon') || desc.includes('animated') || desc.includes('drawing')) tags.push('cartoon', 'animated')
    if (desc.includes('duck') || desc.includes('bird')) tags.push('duck', 'character')

    // Context tags
    if (desc.includes('meme') || desc.includes('funny') || desc.includes('humor')) tags.push('meme', 'humor')
    if (filename.includes('gugo')) tags.push('gugo', 'brand', 'official')

    // Add some defaults
    tags.push('meme', 'character')

    // Remove duplicates and limit
    return [...new Set(tags)].slice(0, 10)
  }

  private generateFallbackAnalysis(filename: string): { description: string; tags: string[] } {
    const lowerFilename = filename.toLowerCase()

    // Basic pattern matching based on filename
    if (lowerFilename.includes('gugo')) {
      return {
        description: 'GUGO-branded meme image - needs manual description for optimal AI matching',
        tags: ['gugo', 'brand', 'meme', 'needs-analysis']
      }
    }

    if (lowerFilename.includes('chart') || lowerFilename.includes('graph')) {
      return {
        description: 'Chart or graph-related meme - likely shows market data or analytical content',
        tags: ['chart', 'analysis', 'data', 'market', 'needs-analysis']
      }
    }

    if (lowerFilename.includes('run') || lowerFilename.includes('fitness')) {
      return {
        description: 'Running or fitness-themed meme - relates to GUGO\'s core running philosophy',
        tags: ['running', 'fitness', 'movement', 'gugo', 'needs-analysis']
      }
    }

    return {
      description: 'Meme image requiring AI analysis for detailed description and contextual tags',
      tags: ['meme', 'needs-analysis', 'unanalyzed']
    }
  }
}

export class OpenAIImageAnalysisProvider implements ImageAnalysisProvider {
  private apiKey: string

  constructor() {
    this.apiKey = process.env.OPENAI_API_KEY || ''
  }

  async analyzeMeme(imagePath: string, filename: string): Promise<AnalysisResult> {
    if (!this.apiKey || this.apiKey === 'your-openai-api-key-here') {
      const fallback = new LocalImageAnalysisProvider()
      const result = await fallback.analyzeMeme(imagePath, filename)
      return {
        ...result,
        statusMessage: 'Fallback analysis (OpenAI API key not configured)'
      }
    }

    try {
      console.log(`Analyzing meme with OpenAI Vision: ${filename}`)

      const fs = require('fs')
      const imageBuffer = fs.readFileSync(imagePath)
      const base64Image = imageBuffer.toString('base64')

      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'gpt-4-vision-preview',
          messages: [
            {
              role: 'user',
              content: [
                {
                  type: 'text',
                  text: `Analyze this meme image and provide:

1. A detailed description (2-3 sentences) explaining:
   - What's visually happening in the image
   - The emotional tone or expression
   - When this meme would be relevant (situational context)
   - Perfect for which types of social media responses

2. 6-10 relevant tags covering:
   - Emotions (happy, frustrated, excited, etc.)
   - Actions (running, pointing, celebrating, etc.)
   - Contexts (crypto, fitness, business, etc.)
   - Situations (fomo, victory, analysis, etc.)

Format your response as JSON:
{
  "description": "detailed description here",
  "tags": ["tag1", "tag2", "tag3", ...]
}

Focus on making the description useful for AI matching - be specific about emotions, situations, and use cases.`
                },
                {
                  type: 'image_url',
                  image_url: {
                    url: `data:image/jpeg;base64,${base64Image}`
                  }
                }
              ]
            }
          ],
          max_tokens: 500,
          temperature: 0.3
        })
      })

      if (!response.ok) {
        throw new Error(`OpenAI API error: ${response.statusText}`)
      }

      const data = await response.json()
      const content = data.choices[0]?.message?.content || ''

      try {
        const parsed = JSON.parse(content)
        return {
          description: parsed.description || 'AI-generated description unavailable',
          tags: Array.isArray(parsed.tags) ? parsed.tags : ['ai-analyzed'],
          isActualAI: true,
          statusMessage: 'Generated by OpenAI GPT-4 Vision'
        }
      } catch (parseError) {
        console.error('Failed to parse OpenAI response:', parseError)
        throw new Error('Invalid response format')
      }

    } catch (error) {
      console.error('OpenAI image analysis error:', error)
      const fallback = new LocalImageAnalysisProvider()
      const result = await fallback.analyzeMeme(imagePath, filename)
      return {
        ...result,
        statusMessage: 'Fallback analysis (OpenAI Vision API error)'
      }
    }
  }
}

export class DeepSeekImageAnalysisProvider implements ImageAnalysisProvider {
  private apiKey: string

  constructor() {
    this.apiKey = process.env.DEEPSEEK_API_KEY || ''
  }

  async analyzeMeme(imagePath: string, filename: string): Promise<AnalysisResult> {
    // DeepSeek doesn't have vision capabilities yet, so fallback to local
    const fallback = new LocalImageAnalysisProvider()
    const result = await fallback.analyzeMeme(imagePath, filename)
    return {
      ...result,
      statusMessage: 'Fallback analysis (DeepSeek vision not available)'
    }
  }
}

export function getImageAnalysisProvider(provider?: string): ImageAnalysisProvider {
  const selectedProvider = provider || 'local'

  switch (selectedProvider) {
    case 'openai':
      return new OpenAIImageAnalysisProvider()
    case 'deepseek':
      return new DeepSeekImageAnalysisProvider()
    case 'local':
    default:
      return new LocalImageAnalysisProvider()
  }
}